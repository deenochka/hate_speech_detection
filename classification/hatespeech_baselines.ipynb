{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fastText import train_supervised, load_model\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(texts):\n",
    "    \"\"\"\n",
    "    Remove RT, urls, screen names, hashtags, punctuation, whitespaces at the end\n",
    "    \"\"\"\n",
    "    preprocessed_texts = []\n",
    "    for text in texts:\n",
    "        if text.startswith ('RT @'):\n",
    "            text = text.replace(\"RT @\", \"@\")\n",
    "        text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "        text = re.sub('(?<![\\w.-])@[A-Za-z][\\w-]+', '', text)\n",
    "        text = re.sub('(?:^|\\s)[ï¼ƒ#]{1}(\\w+)', '', text)\n",
    "        text = text.lower()        \n",
    "        text = text.translate(translator)\n",
    "        text = text.strip()\n",
    "        preprocessed_texts.append(text)\n",
    "    return preprocessed_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(test_y, predicted_y, tags, aver):\n",
    "    \"\"\"\n",
    "    Print main metrics\n",
    "    \"\"\"\n",
    "    return {\"Precision\": metrics.precision_score(test_y, predicted_y, labels=tags, average=aver),\n",
    "            \"Recall\": metrics.recall_score(test_y, predicted_y, labels=tags, average=aver),\n",
    "            \"F1\": metrics.f1_score(test_y, predicted_y, labels=tags, average=aver)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/hate_speech_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13851, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>racism</td>\n",
       "      <td>So Drasko just said he was impressed the girls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>racism</td>\n",
       "      <td>Drasko they didn't cook half a bird you idiot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>racism</td>\n",
       "      <td>Hopefully someone cooks Drasko in the next ep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>racism</td>\n",
       "      <td>of course you were born in serbia...you're as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>racism</td>\n",
       "      <td>These girls are the equivalent of the irritati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                              tweet\n",
       "0  racism  So Drasko just said he was impressed the girls...\n",
       "1  racism  Drasko they didn't cook half a bird you idiot ...\n",
       "2  racism  Hopefully someone cooks Drasko in the next ep ...\n",
       "3  racism  of course you were born in serbia...you're as ...\n",
       "4  racism  These girls are the equivalent of the irritati..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none      8523\n",
       "sexism    4233\n",
       "racism    1093\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove 'bad' rows\n",
    "df = df[(df['label'] == 'sexism') | (df['label'] == 'racism') | (df['label'] == 'none')]\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test set\n",
    "train_df, test_df = model_selection.train_test_split(df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df['label'].values.tolist()\n",
    "train_X = preprocess_tweets(train_df['tweet'].values.tolist())\n",
    "test_y = test_df['label'].values.tolist()\n",
    "test_X = preprocess_tweets(test_df['tweet'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: https://www.aclweb.org/anthology/N16-2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the paper only with character ngrams: Precision 72.87%, Recall 77.75%, F1 Score 73.89."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,4), analyzer='char_wb')),\n",
    "                  ('classifier', LogisticRegression(C=0.2, solver='liblinear', multi_class='ovr'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(train_X, train_y)\n",
    "    \n",
    "predicted_y = model.predict(test_X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': 0.7886690943494444,\n",
       " 'Recall': 0.7799537647637927,\n",
       " 'F1': 0.781568236089799}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_metrics(test_y, predicted_y, tags = np.unique(train_df['label']), aver='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pred:none  pred:racism  pred:sexism\n",
      "true:none          775           21           48\n",
      "true:racism         23           78            8\n",
      "true:sexism        109           18          305\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(metrics.confusion_matrix(test_y, predicted_y, labels=np.unique(train_df['label'])), index=['true:none', 'true:racism', 'true:sexism'], columns=['pred:none', 'pred:racism', 'pred:sexism']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coefs = pd.DataFrame({'coef_score': model.named_steps['classifier'].coef_[0],\n",
    "                            'ngram': model.named_steps['vectorizer'].get_feature_names()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef_score</th>\n",
       "      <th>ngram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>0.829362</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28505</th>\n",
       "      <td>0.659558</td>\n",
       "      <td>xism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>0.647038</td>\n",
       "      <td>exi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17298</th>\n",
       "      <td>0.551652</td>\n",
       "      <td>ment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8711</th>\n",
       "      <td>0.539023</td>\n",
       "      <td>dh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23190</th>\n",
       "      <td>0.528422</td>\n",
       "      <td>rou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13598</th>\n",
       "      <td>0.512266</td>\n",
       "      <td>ian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15907</th>\n",
       "      <td>0.508960</td>\n",
       "      <td>lan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11729</th>\n",
       "      <td>0.485390</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23223</th>\n",
       "      <td>0.484783</td>\n",
       "      <td>rp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef_score ngram\n",
       "2390     0.829362    p \n",
       "28505    0.659558  xism\n",
       "1204     0.647038   exi\n",
       "17298    0.551652  ment\n",
       "8711     0.539023    dh\n",
       "23190    0.528422   rou\n",
       "13598    0.512266  ian \n",
       "15907    0.508960   lan\n",
       "11729    0.485390  for \n",
       "23223    0.484783    rp"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coefs_sorted = model_coefs.sort_values(\"coef_score\", ascending=False)\n",
    "model_coefs_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_fasttext(texts, labels, filepath):\n",
    "    \"\"\"\n",
    "    Prepares the dataset for python fasttext implementation   \n",
    "    \"\"\"    \n",
    "    labels = [\"__label__\"+str(label) for label in labels]    \n",
    "    with open(filepath, 'w') as outfile:\n",
    "        for text, label in zip(texts, labels):\n",
    "            outfile.write(label + \" \" + text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_for_fasttext(train_X, train_y, 'hatespeech_fasttext.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = train_supervised(input='hatespeech_fasttext.txt', epoch=30, thread=10,\n",
    "                        lr=0.1, ws=5, loss='softmax', minCount=5, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_for_fasttext = [text.replace('\\n', '') for text in test_X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = [fasttext_model.predict(text)[0][0].replace(\"__label__\", \"\") for text in test_X_for_fasttext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Precision': 0.7373877399561605,\n",
       " 'Recall': 0.7360351885938142,\n",
       " 'F1': 0.7366371563729935}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_metrics(test_y, predicted_y, tags = np.unique(train_df['label']), aver='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pred:none  pred:racism  pred:sexism\n",
      "true:none          718           23          103\n",
      "true:racism         27           71           11\n",
      "true:sexism        111           16          305\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(metrics.confusion_matrix(test_y, predicted_y, labels=np.unique(train_df['label'])), index=['true:none', 'true:racism', 'true:sexism'], columns=['pred:none', 'pred:racism', 'pred:sexism']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
